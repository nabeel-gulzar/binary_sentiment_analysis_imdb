{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_imdb_nb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFwdTW6TEBiCoFpLBtLw8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabeel-gulzar/binary_sentiment_analysis_imdb/blob/main/sentiment_analysis_imdb_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GziJ3WARYgoZ"
      },
      "source": [
        "%pip install gdown\n",
        "!gdown --id '1HKHlbOmzsOHcjQ7msreZWAtYOTmkha7_'\n",
        "!unzip IMDB_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeQvYo4cf1FS"
      },
      "source": [
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN3PS_lwZ7yk"
      },
      "source": [
        "train_dir = 'Dataset/train/'\n",
        "test_dir = 'Dataset/test/'\n",
        "labels = ['neg', 'pos']\n",
        "stop_words_path = 'Dataset/stop_words.txt'\n",
        "\n",
        "# regex to remove punctuations\n",
        "garbage_words_re = r\"[();:',.\\/@#$%*+!`^_0-9><?\\\"\\-]*\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DFOuatbahD0"
      },
      "source": [
        "class Time(object):\n",
        "    def __init__(self, description):\n",
        "      self.description = description\n",
        "\n",
        "    def __enter__(self): \n",
        "        self.start_time = time()\n",
        "  \n",
        "    def __exit__(self, et, ev, tb):\n",
        "        print(\"Time for {}: {:.4f}\".format(self.description, time()-self.start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJVRb_hHaazG"
      },
      "source": [
        "#reading auxilary data from files\n",
        "with open(stop_words_path, 'r') as file:\n",
        "  stop_words = set()\n",
        "  for word in file.read().split('\\n'):\n",
        "    stop_words.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RxthAYJacxE"
      },
      "source": [
        "def preprocess_review(review):\n",
        "  # split words and filter words that are not in stop words\n",
        "  without_stop_words = ' '.join([token for token in review.lower().split() if token not in stop_words])\n",
        "  return re.sub(garbage_words_re, '', without_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqfI7avvae9c"
      },
      "source": [
        "def get_documents(directory, labels):\n",
        "  sentiments = []\n",
        "  features = []\n",
        "  for i, label in enumSerate(labels):\n",
        "    for filename in glob.iglob(directory+label + '/*.txt', recursive=False):\n",
        "      with open(filename, 'r') as file:\n",
        "        review = file.read()\n",
        "        review = preprocess_review(review)\n",
        "        features.append(review)\n",
        "        sentiments.append(i)\n",
        "  return np.array(features), np.array(sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCNvM_yvmCx"
      },
      "source": [
        "def create_vocabulary(documents):\n",
        "  vocabulary = set()\n",
        "  for document in documents:\n",
        "    tokens = document.split()\n",
        "    [vocabulary.add(w) for w in tokens]\n",
        "  return list(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w6Fxf5LfX0R",
        "outputId": "c31fa3a7-c83d-4864-927c-ba12cb17c70a"
      },
      "source": [
        "with Time(\"Getting Documents\"):\n",
        "  train_documents, train_sentiments = get_documents(train_dir, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Getting Documents: 5.0821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkxGV0ILux22",
        "outputId": "bc749e08-7356-4db7-ea1d-7653e4f12017"
      },
      "source": [
        "with Time(\"Creating Vocabulary\"):\n",
        "  vocabulary = create_vocabulary(train_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Creating Vocabulary: 1.6153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EQgdIA1f51R"
      },
      "source": [
        "def train_naive_bayes(documents, actual_labels, classes, vocabulary):\n",
        "  # total number of documents\n",
        "  total_documents_count = np.size(documents, axis=0)\n",
        "  # initially empty matrix to record liklihood of token against each class \n",
        "  likelihood_matrix = []\n",
        "  # vector to record log of prior\n",
        "  log_prior_vector = np.zeros([len(classes)])\n",
        "  # loop to be run oven the classes (distinct labels)\n",
        "  for class_index in range(len(classes)):\n",
        "    # initialize likelihood for given class with ones\n",
        "    class_likelihood = np.zeros(len(vocabulary))\n",
        "    # filter document belonging the a particular class\n",
        "    class_documents = documents[actual_labels==class_index]\n",
        "    # total number of documents in the particular class\n",
        "    class_documents_count = np.size(class_documents, axis=0)\n",
        "    # log of the proir for the particular class\n",
        "    log_prior_vector[class_index] = (np.log(class_documents_count/total_documents_count))\n",
        "    # concatenation of all the documents in the particular class\n",
        "    conditioned_corpus = ' '.join(class_documents)\n",
        "    # count of distinct tooken (words)\n",
        "    word_counter = Counter(conditioned_corpus.split())\n",
        "    # loop oven the vocabulary to update likehoods\n",
        "    for i, word in enumerate(vocabulary):\n",
        "      # update vector with count of each word in the particular class with index class_index\n",
        "      class_likelihood[i] = word_counter[word]\n",
        "    # compute log likelihood with laplace smooting\n",
        "    class_likelihood = (class_likelihood+1)/(class_likelihood.sum()+len(vocabulary))\n",
        "    # add class likelihood vector to likelihood matrix\n",
        "    likelihood_matrix.append(class_likelihood)\n",
        "  return log_prior_vector, np.log(likelihood_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcE9GuN-oLeG",
        "outputId": "d4e84158-cb0a-44ee-c27a-58613200a505"
      },
      "source": [
        "with Time(\"Training\"):\n",
        "  log_prior, log_likelihood = train_naive_bayes(train_documents, train_sentiments, labels, vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Training: 2.5074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c50ZrfNOlLGX"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RDlZXSUlN6k",
        "outputId": "d9c1979e-23aa-4dd2-920e-ffbe2a8a76de"
      },
      "source": [
        "with Time(\"Getting Test Documents\"):\n",
        "  test_documents, test_sentiments = get_documents(test_dir, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Getting Test Documents: 4.8380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o96ff7gvjuZT"
      },
      "source": [
        "def test_naive_bayes(documents, log_prior, log_likelihood, classes, vocabulary):\n",
        "  total_documents_count = np.size(documents, axis=0)\n",
        "  max_a_posteriori = np.zeros(total_documents_count)\n",
        "  for doc_index, document in enumerate(documents):\n",
        "    posteriori = np.zeros(len(classes))\n",
        "    for class_index in range(len(classes)):\n",
        "      likelihood = log_prior[class_index]\n",
        "      for word in document.split():\n",
        "        likelihood += log_likelihood[class_index].get(word, 0)\n",
        "      posteriori[class_index] = likelihood\n",
        "    max_a_posteriori[doc_index] = np.argmax(posteriori)\n",
        "  return max_a_posteriori"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEIlRBoGoKwt",
        "outputId": "e102bef0-f94d-4b41-9507-7570d087fe5a"
      },
      "source": [
        "log_likelihood_dict = [{}, {}]\n",
        "with Time(\"Converting Likelihoods to Dictionary\"):\n",
        "  for class_index in range(len(labels)):\n",
        "    for i, p in enumerate(log_likelihood[class_index]):\n",
        "      log_likelihood_dict[class_index][vocabulary[i]] = p\n",
        "with Time(\"Predicting on Test\"):\n",
        "  predicted_labels = test_naive_bayes(test_documents, log_prior, log_likelihood_dict, labels, vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Converting Likelihoods to Dictionary: 0.1161\n",
            "Time for Predicting on Test: 4.8782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJqV57y2M6UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec049b1d-c22e-4041-cb18-ab2ec4a48333"
      },
      "source": [
        "with Time(\"Computing Performance Metrics\"):\n",
        "  true_positives = len(predicted_labels[predicted_labels==test_sentiments])\n",
        "  total_documents = len(test_sentiments)\n",
        "  accuracy_score = true_positives/total_documents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Computing Performance Metrics: 0.0081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3nwi9n7TZoT"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ2oZ_qtTbH1",
        "outputId": "b66f6984-1167-4a55-84ef-4319644be0f3"
      },
      "source": [
        "print(\"Accuracy of naive bayes (implemented from scratch) on test data is {:.4f}\".format(accuracy_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of naive bayes (implemented from scratch) on test data is 0.8258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4fClnGApsMW"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m71dpJeNT8K"
      },
      "source": [
        "#imports\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Yu8f4Dpybm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ee010b-5cc3-4647-b57d-327ab3458b5c"
      },
      "source": [
        "# creating feature space i.e bag of words\n",
        "with Time(\"Creating Vocabulary\"):\n",
        "  vectorizer = CountVectorizer().fit(train_documents)\n",
        "# converting train data (tweets) into bow features\n",
        "with Time(\"Transforming Training Doc to Features\"):\n",
        "  train_features = vectorizer.transform(train_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Creating Vocabulary: 3.7877\n",
            "Time for Transforming Training Doc to Features: 3.4732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha9a1w2_qg9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d148c97a-13d7-4fd7-afaf-3e42316ff624"
      },
      "source": [
        "# training multinomial naive bayes on training data\n",
        "with Time(\"Training multinomial model\"):\n",
        "  model = MultinomialNB().fit(train_features, train_sentiments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Training multinomial model: 0.0349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq_NRwzxqpEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a032823-f74c-43d6-d173-914b397cd63d"
      },
      "source": [
        "# converting test data (tweets) into bow features using only train vocabulary\n",
        "with Time(\"Converting Testing Doc to Features\"):\n",
        "  test_features = vectorizer.transform(test_documents)\n",
        "# predicting sentiment of test data\n",
        "with Time(\"Getting Prediction of test\"):\n",
        "  predicted_sentiments = model.predict(test_features)\n",
        "# computing accuracy using sklearn.metrics\n",
        "with Time(\"Computing Performance Metrics\"):\n",
        "  accuracy_score = metrics.accuracy_score(test_sentiments, predicted_sentiments)\n",
        "  # computing confusion matrix using sklearn.metrics\n",
        "  confusion_matrix = metrics.confusion_matrix(test_sentiments, predicted_sentiments, labels=[0,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for Converting Testing Doc to Features: 3.3941\n",
            "Time for Getting Prediction of test: 0.0248\n",
            "Time for Computing Performance Metrics: 0.0210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqTkUpbgSVff"
      },
      "source": [
        "Accuracy for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8RoBxOWtLuz",
        "outputId": "c7fc0b75-cf93-4541-e870-a583a0ecd8df"
      },
      "source": [
        "print(\"Accuracy for test data using sklearn is {:.4f}\".format(accuracy_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for test data using sklearn is 0.8256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx25_ih-SQ3c"
      },
      "source": [
        "Confusion Matrix for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "C10sk3oeRZhR",
        "outputId": "3a3fcc1e-267a-4442-e826-2288c2529874"
      },
      "source": [
        "consfusion_dataframe = pd.DataFrame(data=confusion_matrix, index=[\"Negative\", \"Positive\"], columns=[\"Negative\", \"Positive\"])\n",
        "consfusion_dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Negative</th>\n",
              "      <th>Positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Negative</th>\n",
              "      <td>11002</td>\n",
              "      <td>1498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Positive</th>\n",
              "      <td>2861</td>\n",
              "      <td>9639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Negative  Positive\n",
              "Negative     11002      1498\n",
              "Positive      2861      9639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    }
  ]
}